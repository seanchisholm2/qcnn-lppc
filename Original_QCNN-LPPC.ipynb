{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Computer Modern'; font-size: 42pt; font-weight: bold;\">Quantum Convolutional Neural Network (QCNN) Using *PennyLane*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS / DEPENDENCIES:\n",
    "\n",
    "# PennyLane:\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches # Quantum Circuit Drawings\n",
    "# mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "# from tqdm import tqdm\n",
    "# import csv\n",
    " \n",
    "# import math\n",
    "# import random\n",
    "\n",
    "from scipy.linalg import expm # Unitary-Related Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-weight: bold; font-size: 26pt;\">THE MNIST DATASET</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"qcnn-figures/mnist_plot.png\" alt=\"MNIST Dataset Sample Images\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\">\n",
    "\n",
    "<p style=\"text-align: center; font-family: 'Computer Modern', serif;\">\n",
    "    Sample of the handwritten digital pixelations from the MNIST dataset, which are used for training and testing the QCNN model.<br>\n",
    "    <em>Image source: <a href=\"https://corochann.com/mnist-dataset-introduction-532/\">https://corochann.com/mnist-dataset-introduction-532/</a></em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 16pt; font-weight: bold;\">Loading the MNIST Dataset:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 14pt;\">For our QCNN, we load the MNIST dataset using TorchVision, which allows us to process the data with quantum features and pass it into our neural network. We define the path for the MNIST data directory below, and use TorchVision to load in the MNIST dataset (Note:  the exact \"path name\" that you choose can be arbitrary and/or at your discretion, as our dataloaders will be able to handle the data loading under most root name cases). We then initialize the batch sizes for the MNIST training and testing data sets. In this model, we set the batch size for the training data at 350, and at 250 for the testing data.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant class(es) for MNIST DATA LOADING AND PROCESSING before passing data to QC:\n",
    "from lppc_qcnn.qc_data import DataLPPC as lppc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: torch.Size([350, 1, 28, 28]), dtype: torch.float32\n",
      "test_images shape: torch.Size([250, 1, 28, 28]), dtype: torch.float32\n",
      "train_labels shape: torch.Size([350]), dtype: torch.int64\n",
      "test_labels shape: torch.Size([250]), dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "### READING AND LOADING DATA: \n",
    "### REQUIRED CLASSES: DataLPPC\n",
    "\n",
    "\n",
    "# Set directory for data:\n",
    "data_path = './DATA'\n",
    "\n",
    "# Set batch sizes for training and testing data:\n",
    "batch_train_qcnn = 350\n",
    "batch_test_qcnn = 250\n",
    "\n",
    "# Note: Selections of batch_train=350 and batch_test=250 were chosen for our own preferred sample size, and is\n",
    "# also up to your own discretion.\n",
    "train_images, train_labels, test_images, test_labels = lppc_data.load_mnist_torch(batch_train=batch_train_qcnn,\n",
    "                                                                    batch_test=batch_test_qcnn, root=data_path)\n",
    "\n",
    "# Print relevant shapes and types of your training and testing data to check progress:\n",
    "print(f\"train_images shape: {train_images.shape}, dtype: {train_images.dtype}\")\n",
    "print(f\"test_images shape: {test_images.shape}, dtype: {test_images.dtype}\")\n",
    "print(f\"train_labels shape: {train_labels.shape}, dtype: {train_labels.dtype}\")\n",
    "print(f\"test_labels shape: {test_labels.shape}, dtype: {test_labels.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 16pt; font-weight: bold;\">MNIST DATA TRANSFORMATIONS:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 14pt;\">We initialize the reduction sizes for the MNIST training and testing data sets. In this model, we set the reduction size for the training data at 500, and at 100 for the testing data. We then reduce the number of data points in the training and testing datasets as necessary (Note: it is important to ensure that at least one of the specified reduction values for \"n_train\" and \"n_test\" is smaller than its  corresponding batch size values used during the loading step for the MNIST data, or else no reduction stage is necessary in the steps for the model).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: torch.Size([76, 1, 28, 28]), dtype: torch.float64\n",
      "test_images shape: torch.Size([70, 1, 28, 28]), dtype: torch.float64\n",
      "train_labels shape: torch.Size([76]), dtype: torch.float64\n",
      "test_labels shape: torch.Size([70]), dtype: torch.float64\n"
     ]
    }
   ],
   "source": [
    "### REDUCING THE IMPORTED MNIST DATA\n",
    "### REQUIRED CLASSES: DataLPPC\n",
    "\n",
    "# Reduction sizes:\n",
    "n_train_qcnn = 500\n",
    "n_test_qcnn = 100\n",
    "\n",
    "# Reduce datasets as needed:\n",
    "if n_train_qcnn < batch_train_qcnn or n_test_qcnn < batch_test_qcnn:\n",
    "    train_images, train_labels, test_images, test_labels = lppc_data.mnist_reduce(train_images, train_labels,\n",
    "                                        test_images, test_labels, n_train=n_train_qcnn, n_test=n_test_qcnn)\n",
    "\n",
    "# Print relevant shapes and types of your training and testing data to check progress:\n",
    "print(f\"train_images shape: {train_images.shape}, dtype: {train_images.dtype}\")\n",
    "print(f\"test_images shape: {test_images.shape}, dtype: {test_images.dtype}\")\n",
    "print(f\"train_labels shape: {train_labels.shape}, dtype: {train_labels.dtype}\")\n",
    "print(f\"test_labels shape: {test_labels.shape}, dtype: {test_labels.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: torch.Size([76, 784]), dtype: torch.float64\n",
      "test_images shape: torch.Size([70, 784]), dtype: torch.float64\n"
     ]
    }
   ],
   "source": [
    "### FLATTENING THE IMPORTED MNIST DATA\n",
    "### REQUIRED CLASSES: DataLPPC\n",
    "\n",
    "train_images, test_images = lppc_data.mnist_flatten(train_images, test_images)\n",
    "\n",
    "# Print relevant shapes and types of your training and testing data to check progress:\n",
    "print(f\"train_images shape: {train_images.shape}, dtype: {train_images.dtype}\")\n",
    "print(f\"test_images shape: {test_images.shape}, dtype: {test_images.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (76, 1024), dtype: float64\n",
      "x_test shape: (70, 1024), dtype: float64\n",
      "y_train shape: (76,), dtype: float64\n",
      "y_test shape: (70,), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### PADDING THE FLATTENED DATASETS\n",
    "### REQUIRED CLASSES: DataLPPC\n",
    "\n",
    "x_train, y_train, x_test, y_test = lppc_data.mnist_padding(train_images, train_labels,\n",
    "                                                           test_images, test_labels)\n",
    "\n",
    "# Print relevant shapes and types of your training and testing data to check progress:\n",
    "print(f\"x_train shape: {x_train.shape}, dtype: {x_train.dtype}\")\n",
    "print(f\"x_test shape: {x_test.shape}, dtype: {x_test.dtype}\")\n",
    "print(f\"y_train shape: {y_train.shape}, dtype: {y_train.dtype}\")\n",
    "print(f\"y_test shape: {y_test.shape}, dtype: {y_test.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-weight: bold; font-size: 18pt;\">QCNN MODEL</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 14pt;\">*Discuss QCNN model structure and layering here.*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant classclass(es) for QUANTUM CIRCUIT (before passing weights to the QC):\n",
    "from lppc_qcnn.gellmann_ops import ParamOps as param_ops # PARAMETER OPERATIONS HELPER CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 14pt;\">_Trainable Parameters_:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 14pt;\">We prepare the trainable parameters (weights) for the QCNN by properly transforming their type and shape. We ensure that the weights are Torch tensors of a relevant datatype, and also ensure there are enough weights to train on to be able to pass to the QC, and subsequently through our stochastic gradient descent training loop.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of 'qcnn_weights': 6\n",
      "Shape of 'qcnn_weights': (6,)\n",
      "Length of 'qcnn_weights': 6\n",
      "Type of 'qcnn_weights': <class 'pennylane.numpy.tensor.tensor'>\n",
      "Type of an element of 'qcnn_weights': <class 'pennylane.numpy.tensor.tensor'>\n",
      "Data type of elements in 'qcnn_weights': float64\n"
     ]
    }
   ],
   "source": [
    "### INITIALIZING QUBIT PARAMETERS AND WEIGHTS\n",
    "### REQUIRED CLASSES: ParamOps\n",
    "\n",
    "n_qubits = 2  # Number of qubits\n",
    "# n_qubits = 10\n",
    "active_qubits = 2 # Number of active qubits (same as n_qubits, tracks QC operations)\n",
    "# active_qubits = 10\n",
    "\n",
    "# TODO\n",
    "qcnn_weights0 = np.random.uniform(0, np.pi, size=(n_qubits,1,3))\n",
    "\n",
    "# Prepare weights and transform them by passing to 'param_prep_lppc':\n",
    "qcnn_weights = param_ops.broadcast_weights_lppc(param_ops, qcnn_weights0)\n",
    "\n",
    "# Print relevant attributes of 'qcnn_weights':\n",
    "print(f\"Size of 'qcnn_weights': {qcnn_weights.size}\")\n",
    "print(f\"Shape of 'qcnn_weights': {qcnn_weights.shape}\")\n",
    "print(f\"Length of 'qcnn_weights': {len(qcnn_weights)}\")\n",
    "print(f\"Type of 'qcnn_weights': {type(qcnn_weights)}\")\n",
    "print(f\"Type of an element of 'qcnn_weights': {type(qcnn_weights[0])}\")\n",
    "print(f\"Data type of elements in 'qcnn_weights': {qcnn_weights.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-weight: bold; font-size: 18pt;\">CIRCUIT DRAWING</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant class(es) for CIRCUIT CONSTRUCTION ANF DRAWING prior to visualizations:\n",
    "from lppc_qcnn.qcircuit import QCircuitLPPC as qc_circ # QUANTUM CIRCUIT CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-weight: bold; font-size: 18pt;\">TRAINING / OPTIMIZATION </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mopt_alpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_and_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Update trainable arguments with one step of the optimizer and return the corresponding\n",
      "objective function value prior to the step.\n",
      "\n",
      "Args:\n",
      "    objective_fn (function): the objective function for optimization\n",
      "    *args : variable length argument list for objective function\n",
      "    grad_fn (function): optional gradient function of the\n",
      "        objective function with respect to the variables ``*args``.\n",
      "        If ``None``, the gradient function is computed automatically.\n",
      "        Must return a ``tuple[array]`` with the same number of elements as ``*args``.\n",
      "        Each array of the tuple should have the same shape as the corresponding argument.\n",
      "    **kwargs : variable length of keyword arguments for the objective function\n",
      "\n",
      "Returns:\n",
      "    tuple[list [array], float]: the new variable values :math:`x^{(t+1)}` and the objective\n",
      "    function output prior to the step.\n",
      "    If single arg is provided, list [array] is replaced by array.\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/anaconda3/lib/python3.9/site-packages/pennylane/optimize/gradient_descent.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "# Import relevant class(es) for TRAINING AND OPTIMIZATION-RELATED PROCESSES prior to training weights:\n",
    "from lppc_qcnn.qcircuit import OptStepLPPC as opt_lppc # OPTIMIZATION AND COST CLASS\n",
    "\n",
    "# OPTIMIZER CHECK:\n",
    "# opt_alpha = qml.GradientDescentOptimizer()\n",
    "# opt_alpha.step_and_cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 16pt;\">_Training Model_:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Features must be of length 4; got length 1024. Use the 'pad_with' argument for automated padding.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb Cell 29\u001b[0m line \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Instantiate below for access the model's loss history during training:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# hist_lppc = True\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Training Loop:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     qcnn_weights, loss \u001b[39m=\u001b[39m opt_lppc\u001b[39m.\u001b[39;49mstoch_grad_lppc(opt_lppc, opt, opt_lppc\u001b[39m.\u001b[39;49mmse_cost, qcnn_weights, x_train, y_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m                                                 learning_rate, batch_size, max_iter, conv_tol)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     loss_history\u001b[39m.\u001b[39mappend(loss)  \u001b[39m# Accumulate loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanchisholm/VSCode_LPPC/qcnn-lppc/QCNN-LPPC-Construction_forMNIST.ipynb#X36sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# Print step and cost:\u001b[39;00m\n",
      "File \u001b[0;32m~/VSCode_LPPC/qcnn-lppc/lppc_qcnn/qcircuit.py:930\u001b[0m, in \u001b[0;36mOptStepLPPC.stoch_grad_lppc\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[39mReturns the most recent version of the STOCHASTIC GRADIENT DESCENT optimization\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39mused in the QCNN (CURRENT VERSION: V2).\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[39m# Return Current QC Function ('qcircuit_V2') with appropriate arguments:\u001b[39;00m\n\u001b[0;32m--> 930\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstoch_grad_V2(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/VSCode_LPPC/qcnn-lppc/lppc_qcnn/qcircuit.py:851\u001b[0m, in \u001b[0;36mOptStepLPPC.stoch_grad_V2\u001b[0;34m(self, opt, cost, params, x, y, learning_rate, batch_size, max_iterations, conv_tol, active_qubits, n_qubits)\u001b[0m\n\u001b[1;32m    848\u001b[0m y_batch \u001b[39m=\u001b[39m y[i:i \u001b[39m+\u001b[39m batch_size]\n\u001b[1;32m    850\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iterations):\n\u001b[0;32m--> 851\u001b[0m     params, prev_cost \u001b[39m=\u001b[39m opt\u001b[39m.\u001b[39;49mstep_and_cost(\u001b[39mlambda\u001b[39;49;00m v: cost(\u001b[39mself\u001b[39;49m, v, x_batch, y_batch),\n\u001b[1;32m    852\u001b[0m                                           params)\n\u001b[1;32m    854\u001b[0m     \u001b[39m# Compute Cost for Current Params:\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     batch_cost \u001b[39m=\u001b[39m cost(\u001b[39mself\u001b[39m, params, x_batch, y_batch)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pennylane/optimize/gradient_descent.py:64\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.step_and_cost\u001b[0;34m(self, objective_fn, grad_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_and_cost\u001b[39m(\u001b[39mself\u001b[39m, objective_fn, \u001b[39m*\u001b[39margs, grad_fn\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     45\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Update trainable arguments with one step of the optimizer and return the corresponding\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m    objective function value prior to the step.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m        If single arg is provided, list [array] is replaced by array.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     g, forward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_grad(objective_fn, args, kwargs, grad_fn\u001b[39m=\u001b[39;49mgrad_fn)\n\u001b[1;32m     65\u001b[0m     new_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_grad(g, args)\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m forward \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pennylane/optimize/gradient_descent.py:122\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.compute_grad\u001b[0;34m(objective_fn, args, kwargs, grad_fn)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Compute gradient of the objective function at the given point and return it along with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mthe objective function forward pass (if available).\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39m    will not be evaluted and instead ``None`` will be returned.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m g \u001b[39m=\u001b[39m get_gradient(objective_fn) \u001b[39mif\u001b[39;00m grad_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m grad_fn\n\u001b[0;32m--> 122\u001b[0m grad \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    123\u001b[0m forward \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(g, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m num_trainable_args \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mgetattr\u001b[39m(arg, \u001b[39m\"\u001b[39m\u001b[39mrequires_grad\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pennylane/_grad.py:165\u001b[0m, in \u001b[0;36mgrad.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n\u001b[0;32m--> 165\u001b[0m grad_value, ans \u001b[39m=\u001b[39m grad_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward \u001b[39m=\u001b[39m ans\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m grad_value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/autograd/wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(args[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m argnum)\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m unary_operator(unary_f, x, \u001b[39m*\u001b[39;49mnary_op_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnary_op_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pennylane/_grad.py:183\u001b[0m, in \u001b[0;36mgrad._grad_with_forward\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[39m@unary_to_nary\u001b[39m\n\u001b[1;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_grad_with_forward\u001b[39m(fun, x):\n\u001b[1;32m    180\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This function is a replica of ``autograd.grad``, with the only\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m    difference being that it returns both the gradient *and* the forward pass\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m    value.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     vjp, ans \u001b[39m=\u001b[39m _make_vjp(fun, x)  \u001b[39m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[39mif\u001b[39;00m vspace(ans)\u001b[39m.\u001b[39msize \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    186\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGrad only applies to real scalar-output functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/autograd/core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_vjp\u001b[39m(fun, x):\n\u001b[1;32m      9\u001b[0m     start_node \u001b[39m=\u001b[39m VJPNode\u001b[39m.\u001b[39mnew_root()\n\u001b[0;32m---> 10\u001b[0m     end_value, end_node \u001b[39m=\u001b[39m  trace(start_node, fun, x)\n\u001b[1;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m end_node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(g): \u001b[39mreturn\u001b[39;00m vspace(x)\u001b[39m.\u001b[39mzeros()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/autograd/tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(start_node, fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m trace_stack\u001b[39m.\u001b[39mnew_trace() \u001b[39mas\u001b[39;00m t:\n\u001b[1;32m      9\u001b[0m     start_box \u001b[39m=\u001b[39m new_box(x, t, start_node)\n\u001b[0;32m---> 10\u001b[0m     end_box \u001b[39m=\u001b[39m fun(start_box)\n\u001b[1;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m isbox(end_box) \u001b[39mand\u001b[39;00m end_box\u001b[39m.\u001b[39m_trace \u001b[39m==\u001b[39m start_box\u001b[39m.\u001b[39m_trace:\n\u001b[1;32m     12\u001b[0m         \u001b[39mreturn\u001b[39;00m end_box\u001b[39m.\u001b[39m_value, end_box\u001b[39m.\u001b[39m_node\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/autograd/wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     subargs \u001b[39m=\u001b[39m subvals(args, \u001b[39mzip\u001b[39m(argnum, x))\n\u001b[0;32m---> 15\u001b[0m \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39;49msubargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/VSCode_LPPC/qcnn-lppc/lppc_qcnn/qcircuit.py:851\u001b[0m, in \u001b[0;36mOptStepLPPC.stoch_grad_V2.<locals>.<lambda>\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    848\u001b[0m y_batch \u001b[39m=\u001b[39m y[i:i \u001b[39m+\u001b[39m batch_size]\n\u001b[1;32m    850\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iterations):\n\u001b[0;32m--> 851\u001b[0m     params, prev_cost \u001b[39m=\u001b[39m opt\u001b[39m.\u001b[39mstep_and_cost(\u001b[39mlambda\u001b[39;00m v: cost(\u001b[39mself\u001b[39;49m, v, x_batch, y_batch),\n\u001b[1;32m    852\u001b[0m                                           params)\n\u001b[1;32m    854\u001b[0m     \u001b[39m# Compute Cost for Current Params:\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     batch_cost \u001b[39m=\u001b[39m cost(\u001b[39mself\u001b[39m, params, x_batch, y_batch)\n",
      "File \u001b[0;32m~/VSCode_LPPC/qcnn-lppc/lppc_qcnn/qcircuit.py:756\u001b[0m, in \u001b[0;36mOptStepLPPC.mse_cost\u001b[0;34m(self, params, x, y, active_qubits, n_qubits)\u001b[0m\n\u001b[1;32m    753\u001b[0m     n_qubits \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m    754\u001b[0m \u001b[39m#--------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 756\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqcircuit_lppc(\u001b[39mself\u001b[39m, params, xi) \u001b[39mfor\u001b[39;00m xi \u001b[39min\u001b[39;00m x])\n\u001b[1;32m    758\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean((predictions \u001b[39m-\u001b[39m y) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/VSCode_LPPC/qcnn-lppc/lppc_qcnn/qcircuit.py:756\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    753\u001b[0m     n_qubits \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m    754\u001b[0m \u001b[39m#--------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 756\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqcircuit_lppc(\u001b[39mself\u001b[39;49m, params, xi) \u001b[39mfor\u001b[39;00m xi \u001b[39min\u001b[39;00m x])\n\u001b[1;32m    758\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean((predictions \u001b[39m-\u001b[39m y) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/VSCode_LPPC/qcnn-lppc/lppc_qcnn/qcircuit.py:448\u001b[0m, in \u001b[0;36mQCircuitLPPC.qcircuit_lppc\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39mReturns the most recent version of the QUANTUM CIRCUIT used in the\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39mQCNN (CURRENT VERSION: V2).\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m# Return Current QC Function ('qcircuit_V2') with appropriate arguments:\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqcircuit_V2(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/VSCode_LPPC/qcnn-lppc/lppc_qcnn/qcircuit.py:386\u001b[0m, in \u001b[0;36mQCircuitLPPC.qcircuit_V2\u001b[0;34m(self, params, x, active_qubits, n_qubits, draw)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m#-------------------------------------------------\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[1;32m    384\u001b[0m \u001b[39m# Amplitude Embedding:\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mif\u001b[39;00m draw \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     qml\u001b[39m.\u001b[39;49mAmplitudeEmbedding(x, wires\u001b[39m=\u001b[39;49m\u001b[39mrange\u001b[39;49m(n_qubits), normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    388\u001b[0m \u001b[39m# LAYER 1:\u001b[39;00m\n\u001b[1;32m    389\u001b[0m  \u001b[39m# Convolutional Layer (pass 'params' as argument):\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layer_V2(\u001b[39mself\u001b[39m, params, active_qubits)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pennylane/templates/embeddings/amplitude.py:125\u001b[0m, in \u001b[0;36mAmplitudeEmbedding.__init__\u001b[0;34m(self, features, wires, pad_with, normalize, id)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_with \u001b[39m=\u001b[39m pad_with\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize \u001b[39m=\u001b[39m normalize\n\u001b[0;32m--> 125\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(features, wires, pad_with, normalize)\n\u001b[1;32m    126\u001b[0m \u001b[39msuper\u001b[39m(StatePrep, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(features, wires\u001b[39m=\u001b[39mwires, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39mid\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pennylane/templates/embeddings/amplitude.py:179\u001b[0m, in \u001b[0;36mAmplitudeEmbedding._preprocess\u001b[0;34m(features, wires, pad_with, normalize)\u001b[0m\n\u001b[1;32m    177\u001b[0m dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(wires)\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m pad_with \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m n_features \u001b[39m!=\u001b[39m dim:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeatures must be of length \u001b[39m\u001b[39m{\u001b[39;00mdim\u001b[39m}\u001b[39;00m\u001b[39m; got length \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUse the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_with\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument for automated padding.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m pad_with \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mif\u001b[39;00m n_features \u001b[39m>\u001b[39m dim:\n",
      "\u001b[0;31mValueError\u001b[0m: Features must be of length 4; got length 1024. Use the 'pad_with' argument for automated padding."
     ]
    }
   ],
   "source": [
    "### OPTIMIZATION AND TRAINING\n",
    "### REQUIRED CLASSES: OptStepLPPC, QCircuitLPPC, GellMannOps\n",
    "\n",
    "\n",
    "n_qubits = 2  # Number of qubits\n",
    "active_qubits = 2 # Number of active qubits (same as n_qubits, tracks QC operations)\n",
    "\n",
    "# Initialize the selected optimizer (Note: in this model, the Stochastic Gradient Descent (SGD) Optimizer was \n",
    "# determined to be the most suitable, although the choice of optimizer is additionally up to your own discretion.)\n",
    "\n",
    "# Set value to an integer 1-6 based on desired optimizer selection from 'opt' (Note: For this model, \"1\" corresponds \n",
    "# to the Stochastic Gradient Descent (SGD) Optimizer ('opt_num'=1). You can use qc_opt_print() to see all available\n",
    "# optimizers to choose from.\n",
    "# opt_num_lppc = 1 # TAKE AS PARAMETER\n",
    "\n",
    "# List of all available / acceptable optimizers for QCNN model (Note: this optimizer list does not include\n",
    "# all of the available optimizer selections in PennyLane, it only includes 6 that were selected based on efficiency\n",
    "# and relevance to our model and data):\n",
    "# Optimizer List:\n",
    "#    1: qml.GradientDescentOptimizer, # <- PRIMARY OPTIMIZER\n",
    "#    2: qml.AdamOptimizer,\n",
    "#    3: qml.RMSPropOptimizer,\n",
    "#    4: qml.MomentumOptimizer,\n",
    "#    5: qml.NesterovMomentumOptimizer,\n",
    "#    6: qml.AdagradOptimizer\n",
    "\n",
    "# Select Stochastic Gradient Descent (SGD) Optimizer:\n",
    "# opt = opt_lppc.qcnn_opt_select(opt_num_lppc)\n",
    "opt = qml.GradientDescentOptimizer()\n",
    "\n",
    "# Initialize optimization Parameters:\n",
    "learning_rate = 0.1\n",
    "batch_size = 10\n",
    "max_iter = 100\n",
    "conv_tol = 1e-06\n",
    "\n",
    "# Initialize Training History Parameters:\n",
    "num_steps = 10\n",
    "loss_history = []\n",
    "# Instantiate below for access the model's loss history during training:\n",
    "# hist_lppc = True\n",
    "\n",
    "# Training Loop:\n",
    "for step in range(num_steps):\n",
    "    qcnn_weights, loss = opt_lppc.stoch_grad_lppc(opt_lppc, opt, opt_lppc.mse_cost, qcnn_weights, x_train, y_train,\n",
    "                                                learning_rate, batch_size, max_iter, conv_tol)\n",
    "    \n",
    "    loss_history.append(loss)  # Accumulate loss\n",
    "\n",
    "    # Print step and cost:\n",
    "    print(f\"Step {step}: cost = {loss}\")\n",
    "\n",
    "# Evaluate Optimization Accuracy on testing dataset:\n",
    "predictions = np.array([qc_circ.qcircuit_lppc(qc_circ, qcnn_weights, xi) for xi in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-size: 14pt;\">_Accuracy_:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTIONS AND ACCURACY\n",
    "### REQUIRED CLASSES: OptStepLPPC\n",
    "\n",
    "# Calculate and determine accuracy of the current QCNN model:\n",
    "accuracy = opt_lppc.accuracy_lppc(opt_lppc, predictions, y_test)\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Computer Modern'; font-weight: bold; font-size: 20pt;\">_APPENDIX_</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Computer Modern'; font-size: 10pt; font-weight: bold; text-align: center;\">\n",
    "    © The Laboratory for Particle Physics and Cosmology (LPPC) at Harvard University, Cambridge, MA<br>\n",
    "    © Sean Chisholm<br>\n",
    "    © Pavel Zhelnin\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
